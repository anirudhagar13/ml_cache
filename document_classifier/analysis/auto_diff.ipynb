{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed generator\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\") \n",
    "np.random.seed(5980202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.c\n",
    "N = 2 # dimensions\n",
    "H = 3\n",
    "\n",
    "# random input and output data\n",
    "x = np.random.randn(N,1)\n",
    "y = np.random.randn(N,1)\n",
    "\n",
    "# Randomly initialize weights\n",
    "W1 = np.random.randn(H, N)\n",
    "W2 = np.random.randn(H, H)\n",
    "W3 = np.random.randn(N, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.c.i (Analytic Gradient)\n",
    "# Forward pass \n",
    "M1 = W1@x\n",
    "J = np.maximum(M1,0)\n",
    "M2 = W2@J\n",
    "K = np.maximum(M2,0)\n",
    "M3 = W3@K\n",
    "L = y - M3\n",
    "\n",
    "# Gradient calculation\n",
    "W3_1_grad = -L@K.T\n",
    "temp = -W3.T@L\n",
    "temp[M2<0]=0 # derivative of relu and hadamard prdo\n",
    "W2_1_grad = temp@J.T\n",
    "temp = W2.T@temp\n",
    "temp[M1<0]=0 # derivative of relu\n",
    "W1_1_grad = temp@x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3.c.ii (Backward pass)\n",
    "class Node:\n",
    "    def __init__(self, v=0):\n",
    "        self.v = v\n",
    "        self.d = 0\n",
    "\n",
    "# Creating Computation graph\n",
    "X = Node(x)\n",
    "Y = Node(y)\n",
    "W1_2 = Node(np.array(W1, copy=True))\n",
    "W2_2 = Node(np.array(W2, copy=True))\n",
    "W3_2 = Node(np.array(W3, copy=True))\n",
    "V1, V2 = Node(), Node()\n",
    "V3, V4 = Node(), Node()\n",
    "V5, D, Z = Node(), Node(), Node()\n",
    "\n",
    "# Forward pass\n",
    "V1.v = W1_2.v@X.v\n",
    "V2.v = np.maximum(V1.v,0)\n",
    "V3.v = W2_2.v@V2.v\n",
    "V4.v = np.maximum(V3.v,0)\n",
    "V5.v = W3_2.v@V4.v\n",
    "D.v = Y.v - V5.v\n",
    "Z.v = 0.5*(np.linalg.norm(D.v)**2)\n",
    "\n",
    "# Backward pass\n",
    "Z.d = 1\n",
    "D.d = D.v.T\n",
    "Y.d = 0\n",
    "V5.d = -D.d\n",
    "W3_2.d = V5.d.T@V4.v.T\n",
    "V4.d = W3_2.v.T@V5.d.T\n",
    "temp = V4.d.copy()\n",
    "temp[V3.v < 0] = 0 # derivative of relu\n",
    "V3.d = temp.T\n",
    "W2_2.d = V3.d.T@V2.v.T\n",
    "V2.d = W2_2.v.T@V3.d.T\n",
    "temp = V2.d.copy()\n",
    "temp[V1.v < 0] = 0 # derivative of relu\n",
    "V1.d = temp.T\n",
    "W1_2.d = V1.d.T@X.v.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3.c.iii (Training via Autograd)\n",
    "# Initialize weights, input and output\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x_tensor = torch.tensor(x)\n",
    "y_tensor = torch.tensor(y)\n",
    "\n",
    "W1_3 = torch.tensor(W1, requires_grad=True)\n",
    "W2_3 = torch.tensor(W2, requires_grad=True)\n",
    "W3_3 = torch.tensor(W3, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "A = W1_3.mm(x_tensor).clamp(min=0)\n",
    "B = W2_3.mm(A).clamp(min=0)\n",
    "C = W3_3.mm(B)\n",
    "loss = (C - y_tensor).pow(2).sum()/2\n",
    "\n",
    "# automatic backward pass\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From previous assignment\n",
    "def matrix_comp(A_1, A_2):\n",
    "    # comapres two matrices via subspace projectors\n",
    "    # return np.linalg.norm(A_1@A_1.T - A_2@np.linalg.pinv(A_2))\n",
    "    \n",
    "    # comparing two matrices using relative difference\n",
    "    return np.linalg.norm(A_1 - A_2) / np.linalg.norm(A_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Autograd VS Analytic Gradient\n",
      "W1 Gradient Comparison:  1.9707605543043863e-16\n",
      "W2 Gradient Comparison:  3.0492163003753727e-17\n",
      "W3 Gradient Comparison:  0.0\n"
     ]
    }
   ],
   "source": [
    "print ('Comparing Autograd VS Analytic Gradient')\n",
    "print ('W1 Gradient Comparison: ', matrix_comp(W1_3.grad,W1_1_grad))\n",
    "print ('W2 Gradient Comparison: ', matrix_comp(W2_3.grad,W2_1_grad))\n",
    "print ('W3 Gradient Comparison: ', matrix_comp(W3_3.grad,W3_1_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Autograd VS Reverse Mode Auto Differentiation\n",
      "W1 Gradient Comparison:  1.9707605543043863e-16\n",
      "W2 Gradient Comparison:  3.0492163003753727e-17\n",
      "W3 Gradient Comparison:  0.0\n"
     ]
    }
   ],
   "source": [
    "print ('Comparing Autograd VS Reverse Mode Auto Differentiation')\n",
    "print ('W1 Gradient Comparison: ', matrix_comp(W1_3.grad,W1_2.d))\n",
    "print ('W2 Gradient Comparison: ', matrix_comp(W2_3.grad,W2_2.d))\n",
    "print ('W3 Gradient Comparison: ', matrix_comp(W3_3.grad,W3_2.d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.8147963  -10.00224379]\n",
      " [  1.67837113  -1.71042544]\n",
      " [ -1.89755417   1.93379453]]\n",
      "[[0.         0.         0.        ]\n",
      " [7.95080507 5.96399103 7.56588437]\n",
      " [1.11280508 0.83472799 1.05893108]]\n",
      "[[0.         4.00047362 8.84974739]\n",
      " [0.         3.77378988 8.34828331]]\n"
     ]
    }
   ],
   "source": [
    "# 3ci gradients\n",
    "print (W1_1_grad, W2_1_grad, W3_1_grad, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.8147963  -10.00224379]\n",
      " [  1.67837113  -1.71042544]\n",
      " [ -1.89755417   1.93379453]]\n",
      "[[0.         0.         0.        ]\n",
      " [7.95080507 5.96399103 7.56588437]\n",
      " [1.11280508 0.83472799 1.05893108]]\n",
      "[[0.         4.00047362 8.84974739]\n",
      " [0.         3.77378988 8.34828331]]\n"
     ]
    }
   ],
   "source": [
    "# 3cii gradients\n",
    "print (W1_2.d, W2_2.d, W3_2.d, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  9.8148, -10.0022],\n",
      "        [  1.6784,  -1.7104],\n",
      "        [ -1.8976,   1.9338]], dtype=torch.float64)\n",
      "tensor([[-0.0000, -0.0000, -0.0000],\n",
      "        [7.9508, 5.9640, 7.5659],\n",
      "        [1.1128, 0.8347, 1.0589]], dtype=torch.float64)\n",
      "tensor([[0.0000, 4.0005, 8.8497],\n",
      "        [0.0000, 3.7738, 8.3483]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 3ciii gradients\n",
    "print (W1_3.grad, W2_3.grad, W3_3.grad, sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
