{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.4417216631593655"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random seed generator\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(59802020)\n",
    "n = 50\n",
    "m = 200\n",
    "r = 10\n",
    "\n",
    "# generate a random dataset\n",
    "A = np.random.randn(n, r)\n",
    "B = np.random.randn(r, m)\n",
    "X = np.matmul(A, B) + 0.01*np.random.randn(n, m)\n",
    "\n",
    "# zero centering\n",
    "X -= np.mean(X, axis=-1).reshape(n, 1)\n",
    "A[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 10)\n"
     ]
    }
   ],
   "source": [
    "# 3.a) Calculating best 10 basis for PCA projection\n",
    "\n",
    "def PCAfac(X, dim):\n",
    "    # Accepts centered data matrix and dimentionality to be reduced to\n",
    "    \n",
    "    # performing SVD of the data matrix\n",
    "    eig_mat, Sigma, Vh = np.linalg.svd(X, full_matrices=True)\n",
    "    return eig_mat[:,:dim]\n",
    "    \n",
    "#     # performing eigen value decomposition of covariance matrix of data\n",
    "#     covariance_matrix = np.cov(X)\n",
    "#     Sigma, U1 = np.linalg.eig(covariance_matrix)\n",
    "    \n",
    "#     # The columns of the U are the eigen vectors\n",
    "#     # Order the eigen values and vectors \n",
    "#     ord_U, ord_Sigma = (list(i) for i in zip(*sorted(zip(U, abs(Sigma)), \n",
    "#                                                      key=lambda pair: pair[1],\n",
    "#                                                      reverse=True)))\n",
    "#     # Getting top 'dim' eigen vectors\n",
    "#     proj_U = np.array(ord_U[:dim]).transpose()\n",
    "    \n",
    "#     return proj_U\n",
    "\n",
    "A_1 = PCAfac(X, 10)\n",
    "print (A_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Step-Size: 1.9476773662825538e-07, loss: 421849383.10112387, loss change: -421849383.10112387\n",
      "Epoch: 2, Step-Size: 1.1050624477554588e-06, loss: 24103041.438238394, loss change: 397746341.6628855\n",
      "Epoch: 3, Step-Size: 3.5378831989332355e-07, loss: 1290706.7615710185, loss change: 22812334.676667374\n",
      "Epoch: 4, Step-Size: 1.007538540862914e-06, loss: 790373.9910678442, loss change: 500332.7705031743\n",
      "Epoch: 5, Step-Size: 1.937990911043049e-06, loss: 329275.82738875883, loss change: 461098.1636790854\n",
      "Epoch: 6, Step-Size: 1.186237117677168e-05, loss: 128632.08431156368, loss change: 200643.74307719513\n",
      "Epoch: 7, Step-Size: 1.2048148366788645e-05, loss: 12940.621947460764, loss change: 115691.46236410292\n",
      "Epoch: 8, Step-Size: 4.0616360404385595e-07, loss: 1470.660903467823, loss change: 11469.961043992942\n",
      "Epoch: 9, Step-Size: 1.0245705735157337e-05, loss: 1430.6679019659894, loss change: 39.99300150183353\n",
      "Epoch: 10, Step-Size: 6.114725378450414e-07, loss: 668.6204090370566, loss change: 762.0474929289328\n",
      "Epoch: 11, Step-Size: 7.1628383959469954e-06, loss: 644.5771086308276, loss change: 24.043300406228923\n",
      "Epoch: 12, Step-Size: 4.826172595263474e-05, loss: 398.3422186512235, loss change: 246.23488997960413\n",
      "Epoch: 13, Step-Size: 3.595732351526515e-05, loss: 177.3157070405166, loss change: 221.0265116107069\n",
      "Epoch: 14, Step-Size: 1.2836757600663587e-05, loss: 87.13793750855068, loss change: 90.17776953196592\n",
      "Epoch: 15, Step-Size: 1.361792456729021e-05, loss: 18.27096349111413, loss change: 68.86697401743655\n",
      "Epoch: 16, Step-Size: 5.982612105721909e-06, loss: 4.352837670417993, loss change: 13.918125820696137\n",
      "Epoch: 17, Step-Size: 3.6571924265757856e-06, loss: 2.2945536828779685, loss change: 2.0582839875400247\n",
      "Epoch: 18, Step-Size: 1.5387675522863074e-05, loss: 1.8432684045488075, loss change: 0.451285278329161\n",
      "Epoch: 19, Step-Size: 2.224624760257541e-05, loss: 0.9098022061878427, loss change: 0.9334661983609648\n",
      "Epoch: 20, Step-Size: 2.1465401982399276e-06, loss: 0.7962153416996846, loss change: 0.11358686448815813\n",
      "Epoch: 21, Step-Size: 7.660037027351618e-06, loss: 0.7834877095582639, loss change: 0.012727632141420697\n",
      "Epoch: 22, Step-Size: 1.403930278962036e-05, loss: 0.7693637764009835, loss change: 0.014123933157280333\n",
      "Epoch: 23, Step-Size: 1.433698054881925e-05, loss: 0.7684861499421978, loss change: 0.0008776264587857519\n",
      "Epoch: 24, Step-Size: 2.897959550637194e-06, loss: 0.7683141460363242, loss change: 0.0001720039058735745\n",
      "Epoch: 25, Step-Size: 1.4865066684956137e-07, loss: 0.7681804249571416, loss change: 0.00013372107918263598\n",
      "Epoch: 26, Step-Size: 3.316109091680628e-06, loss: 0.7681773656051425, loss change: 3.0593519990684825e-06\n",
      "Epoch: 27, Step-Size: 8.633874398520283e-06, loss: 0.7681252252420602, loss change: 5.2140363082320285e-05\n",
      "Epoch: 28, Step-Size: 1.4372639479492279e-05, loss: 0.768091694622694, loss change: 3.35306193661733e-05\n",
      "Epoch: 29, Step-Size: 8.592706704400874e-06, loss: 0.7680775719415255, loss change: 1.4122681168515783e-05\n",
      "Epoch: 30, Step-Size: 5.118975016332621e-06, loss: 0.7680735353106557, loss change: 4.0366308697858955e-06\n",
      "Epoch: 31, Step-Size: 4.874096314528381e-05, loss: 0.7680725743984418, loss change: 9.609122139098147e-07\n",
      "Epoch: 32, Step-Size: 6.411957257213475e-06, loss: 0.7680717701715185, loss change: 8.042269232788612e-07\n",
      "Epoch: 33, Step-Size: 6.040235669466752e-06, loss: 0.7680711772276366, loss change: 5.929438818830235e-07\n",
      "Epoch: 34, Step-Size: 2.9743908185444318e-05, loss: 0.7680710148839024, loss change: 1.6234373423351656e-07\n",
      "Epoch: 35, Step-Size: 1.0334539387738041e-16, loss: 0.7680707459247375, loss change: 2.6895916493963057e-07\n",
      "Epoch: 36, Step-Size: 5.504165290816916e-06, loss: 0.7680707459247372, loss change: 2.220446049250313e-16\n",
      "Epoch: 37, Step-Size: 1.3710175012268877e-05, loss: 0.7680707446033566, loss change: 1.321380671548411e-09\n",
      "Epoch: 38, Step-Size: 2.4862353859804136e-05, loss: 0.7680707429089012, loss change: 1.6944553538067453e-09\n",
      "Epoch: 39, Step-Size: 5.907129362643843e-06, loss: 0.7680707423500185, loss change: 5.58882717882625e-10\n",
      "Epoch: 40, Step-Size: 4.8339196066513786e-05, loss: 0.7680707423356156, loss change: 1.4402923298462156e-11\n",
      "Epoch: 41, Step-Size: 1.6527051166507384e-06, loss: 0.7680707423065996, loss change: 2.9016011815485854e-11\n",
      "Epoch: 42, Step-Size: 1.3611876354130447e-05, loss: 0.7680707423052027, loss change: 1.396882609583372e-12\n",
      "Epoch: 43, Step-Size: 6.1277678486236156e-06, loss: 0.7680707423035099, loss change: 1.6927570456459762e-12\n",
      "Epoch: 44, Step-Size: 2.2075892570491857e-05, loss: 0.7680707423016542, loss change: 1.855737785660949e-12\n",
      "Epoch: 45, Step-Size: 1.0457689434826041e-07, loss: 0.7680707423009736, loss change: 6.80566714095221e-13\n",
      "Epoch: 46, Step-Size: 4.853340010451114e-09, loss: 0.7680707423009694, loss change: 4.218847493575595e-15\n",
      "Epoch: 47, Step-Size: 5.736055575420839e-06, loss: 0.7680707423009688, loss change: 5.551115123125783e-16\n",
      "Epoch: 48, Step-Size: 1.1006372545892318e-06, loss: 0.7680707423008507, loss change: 1.1812772982011666e-13\n",
      "Epoch: 49, Step-Size: 3.1055506142942462e-12, loss: 0.7680707423008373, loss change: 1.3433698597964394e-14\n",
      "Epoch: 50, Step-Size: 5.571536203150338e-13, loss: 0.768070742300836, loss change: 1.2212453270876722e-15\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3.b) Backtracking line search gradient descent to get encoder matrix\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_gradient(A, X):\n",
    "    # update and get gradient value as calculated in 3.b.i (-4(X - AA^TX)(X^TA))\n",
    "    return -4*(X - A@(A.T)@X)@((X.T)@A)\n",
    "\n",
    "def backtrack_line_search(A, X):\n",
    "    # Uses function and some random values to get appropriate step size\n",
    "    # f(X + t*DeltaX) - f(X) >= ct<grad_X, Delta_X>\n",
    "    # c belongs to [0,1], rho belongs to [0,1]\n",
    "    t = 1\n",
    "    c = random.uniform(0, 1)\n",
    "    rho = random.uniform(0, 1)\n",
    "    grad_A = get_gradient(A, X)\n",
    "    f_A = np.linalg.norm(X - A@(A.T)@X)**2\n",
    "    while True:\n",
    "        f_A_grad = np.linalg.norm(X - (A - t*grad_A)@((A - t*grad_A).T)@X)**2\n",
    "        exp_1 = f_A_grad - f_A \n",
    "        exp_2 = -c*t*np.sum(grad_A*grad_A)\n",
    "        if exp_1 >= exp_2:\n",
    "            t *= rho\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    return t\n",
    "\n",
    "def gradient_descent(A, X):\n",
    "    # performs gradient descent using backtracking line search\n",
    "    N = 50\n",
    "    loss = 0\n",
    "    loss_prev = 0\n",
    "    epoch = 0\n",
    "    loss_plot = [list(), list()]\n",
    "    while epoch < N:\n",
    "        # get new loss\n",
    "        epoch += 1\n",
    "        loss = np.linalg.norm(X - A@A.T@X)**2\n",
    "        \n",
    "        # get_direction\n",
    "        grad_A = get_gradient(A, X)\n",
    "        \n",
    "        # get size\n",
    "        t = backtrack_line_search(A, X)\n",
    "        \n",
    "        # update step\n",
    "        A -= t*grad_A\n",
    "        \n",
    "        # logs\n",
    "        print ('Epoch: {0}, Step-Size: {1}, loss: {2}, loss change: {3}'.format(epoch, t, loss, loss_prev-loss))\n",
    "        loss_prev = loss\n",
    "        \n",
    "        # for plotting a graph\n",
    "        loss_plot[0].append(epoch)\n",
    "        loss_plot[1].append(loss)\n",
    "       \n",
    "    plt.plot(loss_plot[0], loss_plot[1])\n",
    "    plt.show()\n",
    "    return A\n",
    "  \n",
    "A_copy = copy.deepcopy(A)\n",
    "A_2 = gradient_descent(A_copy, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between A1 and A2:  1.1894858011840334e-08\n"
     ]
    }
   ],
   "source": [
    "# 3.c) Comparing two matrices\n",
    "def matrix_comp(A_1, A_2):\n",
    "    # comapres two matrices via subspace projectors\n",
    "    return np.linalg.norm(A_1@A_1.T - A_2@np.linalg.pinv(A_2))\n",
    "\n",
    "print ('Difference between A1 and A2: ',matrix_comp(A_1, A_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Step-A: 5.4e-05, Step-Z: 0.001419, loss: 5038406.436923583, loss_change:-5038406.436923583\n",
      "Epoch: 2, Step-A: 4e-06, Step-Z: 0.000671, loss: 1503986.0413449497, loss_change:3534420.395578633\n",
      "Epoch: 3, Step-A: 3.1e-05, Step-Z: 0.003637, loss: 1058930.744036602, loss_change:445055.29730834765\n",
      "Epoch: 4, Step-A: 3.5e-05, Step-Z: 0.078723, loss: 106224.73763898108, loss_change:952706.006397621\n",
      "Epoch: 5, Step-A: 5.6e-05, Step-Z: 0.003299, loss: 31763.6340602806, loss_change:74461.10357870048\n",
      "Epoch: 6, Step-A: 3.7e-05, Step-Z: 0.036979, loss: 21569.519932967047, loss_change:10194.114127313554\n",
      "Epoch: 7, Step-A: 1.4e-05, Step-Z: 0.1208, loss: 11906.938985235676, loss_change:9662.58094773137\n",
      "Epoch: 8, Step-A: 0.000137, Step-Z: 0.060002, loss: 9851.482222989209, loss_change:2055.4567622464674\n",
      "Epoch: 9, Step-A: 6.8e-05, Step-Z: 0.08047, loss: 7745.422580687753, loss_change:2106.059642301456\n",
      "Epoch: 10, Step-A: 3.4e-05, Step-Z: 0.003578, loss: 6783.536776160264, loss_change:961.8858045274892\n",
      "Epoch: 11, Step-A: 5.7e-05, Step-Z: 0.149603, loss: 6519.649323559046, loss_change:263.8874526012178\n",
      "Epoch: 12, Step-A: 7e-06, Step-Z: 0.494597, loss: 5652.685174883844, loss_change:866.9641486752016\n",
      "Epoch: 13, Step-A: 4.9e-05, Step-Z: 0.1044, loss: 5344.961710324789, loss_change:307.7234645590552\n",
      "Epoch: 14, Step-A: 5.5e-05, Step-Z: 0.08601, loss: 4920.523370788833, loss_change:424.4383395359564\n",
      "Epoch: 15, Step-A: 6.8e-05, Step-Z: 0.016517, loss: 4441.670462758784, loss_change:478.85290803004864\n",
      "Epoch: 16, Step-A: 4.5e-05, Step-Z: 0.071949, loss: 3729.4170498848985, loss_change:712.2534128738857\n",
      "Epoch: 17, Step-A: 0.000174, Step-Z: 0.088005, loss: 3353.2672670312313, loss_change:376.14978285366715\n",
      "Epoch: 18, Step-A: 1.8e-05, Step-Z: 0.068016, loss: 3167.0961507283805, loss_change:186.1711163028508\n",
      "Epoch: 19, Step-A: 0.000405, Step-Z: 0.478679, loss: 2090.7291318474445, loss_change:1076.367018880936\n",
      "Epoch: 20, Step-A: 5.6e-05, Step-Z: 0.020282, loss: 1169.8258425188624, loss_change:920.9032893285821\n",
      "Epoch: 21, Step-A: 4.6e-05, Step-Z: 0.006949, loss: 241.81134008826453, loss_change:928.0145024305979\n",
      "Epoch: 22, Step-A: 4.8e-05, Step-Z: 0.150362, loss: 64.92088964437526, loss_change:176.89045044388928\n",
      "Epoch: 23, Step-A: 4.3e-05, Step-Z: 0.082688, loss: 24.628553986392888, loss_change:40.29233565798238\n",
      "Epoch: 24, Step-A: 3.5e-05, Step-Z: 0.028884, loss: 5.410217018572919, loss_change:19.21833696781997\n",
      "Epoch: 25, Step-A: 0.000129, Step-Z: 0.093796, loss: 2.152317856957353, loss_change:3.2578991616155655\n",
      "Epoch: 26, Step-A: 8.7e-05, Step-Z: 0.043169, loss: 1.5242906120351238, loss_change:0.6280272449222293\n",
      "Epoch: 27, Step-A: 5e-06, Step-Z: 0.113277, loss: 1.873575400579958, loss_change:-0.34928478854483425\n",
      "Epoch: 28, Step-A: 4.8e-05, Step-Z: 0.095836, loss: 0.9923743810886859, loss_change:0.8812010194912722\n",
      "Epoch: 29, Step-A: 1.6e-05, Step-Z: 0.043753, loss: 0.9843756821773365, loss_change:0.007998698911349411\n",
      "Epoch: 30, Step-A: 0.000128, Step-Z: 0.084026, loss: 0.813795471256789, loss_change:0.17058021092054754\n",
      "Epoch: 31, Step-A: 1.7e-05, Step-Z: 0.01286, loss: 0.8090247039359545, loss_change:0.004770767320834479\n",
      "Epoch: 32, Step-A: 5.9e-05, Step-Z: 0.204273, loss: 0.7851079836772291, loss_change:0.023916720258725377\n",
      "Epoch: 33, Step-A: 2.7e-05, Step-Z: 0.055119, loss: 0.8137506339803209, loss_change:-0.028642650303091766\n",
      "Epoch: 34, Step-A: 5.7e-05, Step-Z: 0.160262, loss: 0.772204026139864, loss_change:0.04154660784045683\n",
      "Epoch: 35, Step-A: 1.3e-05, Step-Z: 0.12411, loss: 0.7776774609007644, loss_change:-0.005473434760900342\n",
      "Epoch: 36, Step-A: 5e-06, Step-Z: 0.044081, loss: 0.772294630409658, loss_change:0.005382830491106438\n",
      "Epoch: 37, Step-A: 8.6e-05, Step-Z: 0.083991, loss: 0.7690771974254974, loss_change:0.003217432984160551\n",
      "Epoch: 38, Step-A: 4.3e-05, Step-Z: 0.110946, loss: 0.7705588924816464, loss_change:-0.001481695056149035\n",
      "Epoch: 39, Step-A: 4e-05, Step-Z: 0.020687, loss: 0.7715562137488332, loss_change:-0.0009973212671867682\n",
      "Epoch: 40, Step-A: 4.7e-05, Step-Z: 0.097674, loss: 0.7682231238674032, loss_change:0.003333089881429996\n",
      "Epoch: 41, Step-A: 1.1e-05, Step-Z: 0.124666, loss: 0.7682289623864307, loss_change:-5.838519027467903e-06\n",
      "Epoch: 42, Step-A: 1.1e-05, Step-Z: 0.050225, loss: 0.7681416831577751, loss_change:8.727922865559012e-05\n",
      "Epoch: 43, Step-A: 5.4e-05, Step-Z: 0.159579, loss: 0.7680885487369137, loss_change:5.313442086141151e-05\n",
      "Epoch: 44, Step-A: 6e-06, Step-Z: 0.063293, loss: 0.7680852665872921, loss_change:3.2821496215307278e-06\n",
      "Epoch: 45, Step-A: 3e-05, Step-Z: 0.205387, loss: 0.7680761781710729, loss_change:9.0884162192717e-06\n",
      "Epoch: 46, Step-A: 2.5e-05, Step-Z: 0.002915, loss: 0.768073993918696, loss_change:2.1842523768578204e-06\n",
      "Epoch: 47, Step-A: 4.3e-05, Step-Z: 0.045044, loss: 0.7680729622966547, loss_change:1.0316220413475463e-06\n",
      "Epoch: 48, Step-A: 0.000109, Step-Z: 0.173097, loss: 0.7680722776137248, loss_change:6.846829299078649e-07\n",
      "Epoch: 49, Step-A: 4.4e-05, Step-Z: 0.060234, loss: 0.7680716200108532, loss_change:6.57602871600993e-07\n",
      "Epoch: 50, Step-A: 6.5e-05, Step-Z: 0.058322, loss: 0.7680712914462534, loss_change:3.285645997497255e-07\n",
      "Epoch: 51, Step-A: 5e-05, Step-Z: 0.034767, loss: 0.7680712176196096, loss_change:7.382664379829151e-08\n",
      "Epoch: 52, Step-A: 1.7e-05, Step-Z: 0.03392, loss: 0.7680709933119533, loss_change:2.2430765633796312e-07\n",
      "Epoch: 53, Step-A: 0.000118, Step-Z: 0.018235, loss: 0.7680709098408367, loss_change:8.347111657425188e-08\n",
      "Epoch: 54, Step-A: 5.2e-05, Step-Z: 0.023231, loss: 0.768070834598716, loss_change:7.524212075615822e-08\n",
      "Epoch: 55, Step-A: 7.9e-05, Step-Z: 0.015299, loss: 0.7680708041575496, loss_change:3.044116636541361e-08\n",
      "Epoch: 56, Step-A: 5.5e-05, Step-Z: 0.071105, loss: 0.7680707824354427, loss_change:2.172210689099785e-08\n",
      "Epoch: 57, Step-A: 0.00012, Step-Z: 0.001429, loss: 0.7680707720728673, loss_change:1.0362575353362047e-08\n",
      "Epoch: 58, Step-A: 6e-06, Step-Z: 0.00203, loss: 0.768070765003986, loss_change:7.068881369143298e-09\n",
      "Epoch: 59, Step-A: 8.4e-05, Step-Z: 0.129051, loss: 0.7680707618500306, loss_change:3.1539553191350933e-09\n",
      "Epoch: 60, Step-A: 3.6e-05, Step-Z: 0.025863, loss: 0.7680707889777776, loss_change:-2.7127746937694042e-08\n",
      "Epoch: 61, Step-A: 3e-06, Step-Z: 0.147913, loss: 0.7680707489029933, loss_change:4.007478426881761e-08\n",
      "Epoch: 62, Step-A: 6e-05, Step-Z: 0.008317, loss: 0.7680707484196143, loss_change:4.833790034908247e-10\n",
      "Epoch: 63, Step-A: 3.7e-05, Step-Z: 0.030922, loss: 0.7680707465753746, loss_change:1.8442397609064187e-09\n",
      "Epoch: 64, Step-A: 0.000368, Step-Z: 0.297175, loss: 0.7680707457512128, loss_change:8.241617388549116e-10\n",
      "Epoch: 65, Step-A: 5.9e-05, Step-Z: 0.013664, loss: 0.7680707426570333, loss_change:3.094179468199343e-09\n",
      "Epoch: 66, Step-A: 9e-06, Step-Z: 0.075217, loss: 0.7680707424133109, loss_change:2.4372248663695473e-10\n",
      "Epoch: 67, Step-A: 0.000117, Step-Z: 0.092263, loss: 0.7680707423233984, loss_change:8.991241084999047e-11\n",
      "Epoch: 68, Step-A: 5.3e-05, Step-Z: 0.083748, loss: 0.768070742312736, loss_change:1.0662470906197541e-11\n",
      "Epoch: 69, Step-A: 6.4e-05, Step-Z: 0.050133, loss: 0.7680707423108574, loss_change:1.8786083799682274e-12\n",
      "Epoch: 70, Step-A: 2.4e-05, Step-Z: 0.054025, loss: 0.7680707423091565, loss_change:1.7008616737257398e-12\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4.b) Backtracking line search subgradient descent for two parameters\n",
    "\n",
    "def get_gradient_A(A, Z, X):\n",
    "    # update and get gradient value as calculated in 3.b.i (-2(X - AZ)(Z^T))\n",
    "    return -2*(X - A@Z)@(Z.T)\n",
    "\n",
    "def get_gradient_Z(A, Z, X):\n",
    "    # update and get gradient value as calculated in 3.b.i (-2(A^T)(X - AZ))\n",
    "    return -2*(A.T)@(X - A@Z)\n",
    "\n",
    "def backtrack_line_search(A, Z, X, var):\n",
    "    # Uses function and some random values to get appropriate step size\n",
    "    # f(X + t*DeltaX) - f(X) >= ct<grad_X, Delta_X>\n",
    "    # c belongs to [0,1], rho belongs to [0,1]\n",
    "    t = 1\n",
    "    c = random.uniform(0, 1)\n",
    "    rho = random.uniform(0, 1)\n",
    "    gradient = get_gradient_Z(A, Z, X)\n",
    "    if var == 'A':\n",
    "        gradient = get_gradient_A(A, Z, X)\n",
    "    f_A_Z = np.linalg.norm(X - A@Z)**2\n",
    "    while True:\n",
    "        f_A_Z_del = 0\n",
    "        if var == 'A':    \n",
    "            f_A_Z_del = np.linalg.norm(X - (A - t*gradient)@Z)**2\n",
    "        else:\n",
    "            f_A_Z_del = np.linalg.norm(X - A@(Z - t*gradient))**2\n",
    "            \n",
    "        exp_1 = f_A_Z_del - f_A_Z \n",
    "        exp_2 = -c*t*np.sum(gradient*gradient)\n",
    "        if exp_1 >= exp_2:\n",
    "            t *= rho\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    return t\n",
    "\n",
    "def sub_gradient_descent(A, Z, X):\n",
    "    # performs sub gradient descent of two variables using backtracking line search\n",
    "    N = 70\n",
    "    loss = 0\n",
    "    loss_prev = 0\n",
    "    epoch = 0\n",
    "    loss_plot = [list(), list()]\n",
    "    # Can even give condit\n",
    "    while epoch < N:\n",
    "        # get new loss\n",
    "        epoch += 1\n",
    "        loss = np.linalg.norm(X - A@Z)**2\n",
    "        \n",
    "        # get_direction_A\n",
    "        grad_A = get_gradient_A(A, Z, X)\n",
    "        \n",
    "        # get_direction_Z\n",
    "        grad_Z = get_gradient_Z(A, Z, X)\n",
    "        \n",
    "        # get size A\n",
    "        t_A = backtrack_line_search(A, Z, X, 'A')\n",
    "        \n",
    "        # get size Z\n",
    "        t_Z = backtrack_line_search(A, Z, X, 'Z')\n",
    "        \n",
    "        # update step A\n",
    "        A -= t_A*grad_A\n",
    "        \n",
    "        # update step Z\n",
    "        Z -= t_Z*grad_Z\n",
    "        \n",
    "        # logs\n",
    "        print ('Epoch: {0}, Step-A: {1}, Step-Z: {2}, loss: {3}, loss_change:{4}'.format(epoch, round(t_A,6), round(t_Z,6), loss, (loss_prev - loss)))\n",
    "        loss_prev = loss\n",
    "        \n",
    "        # for plotting a graph\n",
    "        loss_plot[0].append(epoch)\n",
    "        loss_plot[1].append(loss)\n",
    "       \n",
    "    plt.plot(loss_plot[0], loss_plot[1])\n",
    "    plt.show()\n",
    "    return A\n",
    "  \n",
    "A_copy = copy.deepcopy(A)\n",
    "Z = A_1.T@X\n",
    "A_3 = sub_gradient_descent(A_copy, Z, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between A1 and A3:  3.479398829231894e-08\n",
      "Difference between A2 and A3:  4.132234436413547e-08\n"
     ]
    }
   ],
   "source": [
    "# 4.c) Comparing two matrices\n",
    "def matrix_comp(A_1, A_2):\n",
    "    # comapres two matrices via subspace projectors\n",
    "    return np.linalg.norm(A_1@A_1.T - A_2@np.linalg.pinv(A_2))\n",
    "\n",
    "print ('Difference between A1 and A3: ',matrix_comp(A_1, A_3))\n",
    "print ('Difference between A2 and A3: ',matrix_comp(A_2, A_3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
